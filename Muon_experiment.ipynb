{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFkZnDTmLGY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1KfubwlK_R-",
        "outputId": "6db8bc92-1f69-4257-e31a-3a0333b0371c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Muon'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 106 (delta 25), reused 100 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (106/106), 41.75 KiB | 5.96 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Elgohary74/Muon.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Muon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI4naQhKLON4",
        "outputId": "909a4409-d276-415f-e6d1-2bdd107a0a04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Muon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WKr-ozxLUil",
        "outputId": "2c7aaa19-7b74-4f09-cbd8-f04a5454773c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchtune in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: torchdata==0.11.0 in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (0.3.13)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (0.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.12/dist-packages (from torchtune->-r requirements.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.11.0->torchtune->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.12/dist-packages (from blobfile>=2->torchtune->-r requirements.txt (line 4)) (3.23.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.12/dist-packages (from blobfile>=2->torchtune->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]->torchtune->-r requirements.txt (line 4)) (0.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->torchtune->-r requirements.txt (line 4)) (4.9.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.22.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd experiments/exp1_muon_vs_adam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZIY5ihRLX-6",
        "outputId": "e3ab5146-418e-45ad-90e0-a59e3736fdfd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Muon/experiments/exp1_muon_vs_adam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_experiments.py -e muon_baseline adam_baseline adam_higher_lr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UadQlQS9Ldrs",
        "outputId": "afda756d-0974-4371-93c5-7d9c90253ea5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-16 22:00:52 | INFO     | root | Logging to file: logs/training_20251216_220052.log\n",
            "2025-12-16 22:00:52 | INFO     | root | Running 3 experiments: ['muon_baseline', 'adam_baseline', 'adam_higher_lr']\n",
            "ðŸŒ± Set all seeds to 42\n",
            "Loading dataset with Hugging Face Datasets API...\n",
            "Loading raw dataset and splitting documents...\n",
            "README.md: 7.05kB [00:00, 26.9MB/s]\n",
            "Resolving data files: 100% 104/104 [00:00<00:00, 380.96it/s]\n",
            "Resolving data files: 100% 104/104 [00:00<00:00, 30651.93it/s]\n",
            "Split into 1,800 train docs and 200 val docs\n",
            "2025-12-16 22:00:59 | INFO     | data.loader | Loading tokenizer: HuggingFaceTB/SmolLM-135M\n",
            "tokenizer_config.json: 3.69kB [00:00, 21.6MB/s]\n",
            "vocab.json: 801kB [00:00, 43.8MB/s]\n",
            "merges.txt: 466kB [00:00, 113MB/s]\n",
            "tokenizer.json: 2.10MB [00:00, 171MB/s]\n",
            "special_tokens_map.json: 100% 831/831 [00:00<00:00, 6.29MB/s]\n",
            "2025-12-16 22:01:00 | INFO     | data.loader | Set pad_token to eos_token: <|endoftext|>\n",
            "Tokenizing train set...\n",
            "2025-12-16 22:01:00 | INFO     | data.loader | Tokenizing dataset...\n",
            "Tokenizing: 100% 1800/1800 [00:04<00:00, 394.13 examples/s]\n",
            "2025-12-16 22:01:05 | INFO     | data.loader | Grouping texts into blocks of size 512 with stride 512\n",
            "Grouping texts: 100% 1800/1800 [00:01<00:00, 1506.26 examples/s]\n",
            "Map: 100% 2516/2516 [00:00<00:00, 2845.03 examples/s]\n",
            "Tokenizing validation set...\n",
            "2025-12-16 22:01:07 | INFO     | data.loader | Tokenizing dataset...\n",
            "Tokenizing: 100% 200/200 [00:00<00:00, 499.48 examples/s]\n",
            "2025-12-16 22:01:07 | INFO     | data.loader | Grouping texts into blocks of size 512 with stride 512\n",
            "Grouping texts: 100% 200/200 [00:00<00:00, 1504.59 examples/s]\n",
            "Map: 100% 283/283 [00:00<00:00, 2992.56 examples/s]\n",
            "Train sequences: 2,516, Val sequences: 283\n",
            "\n",
            "================================================================================\n",
            "Starting experiment 1/3: muon_baseline\n",
            "Starting experiment 1/3: muon_baseline\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            " Running Experiment: muon_baseline\n",
            "================================================================================\n",
            "Description: Baseline: Hybrid Muon (2D weights) + AdamW (embeddings/norms) - LR 0.07 (optimized)\n",
            "Optimizer: muon_hybrid\n",
            "Output: muon_baseline\n",
            "================================================================================\n",
            "\n",
            "ðŸŒ± Set all seeds to 42\n",
            "   Total parameters: 79,059,840\n",
            "   Active parameters: 22,436,736\n",
            "  Expert parameters: 56,623,104\n",
            "  Parameter efficiency: 28.4% active per forward pass\n",
            "  Muon parameters: 60,180,480\n",
            "  AdamW parameters: 18,879,360\n",
            "  Muon config: LR=0.07, momentum=0.95, nesterov=True, ns_steps=5\n",
            "   Using Muon (hybrid) optimizer\n",
            "Training muon_baseline:   0% 0/500 [00:01<?, ?it/s, loss=10.7964, aux=0.0602, acc=1.000, ppl=48844.1, lr=0.00000]W1216 22:01:18.225000 1376 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\n",
            "Step 10: Val Loss: 10.7258, Val Acc: 0.0164, Val PPL: 45516.03, LR: 0.00560\n",
            "Training muon_baseline:   4% 20/500 [00:35<14:01,  1.75s/it, loss=10.7964, aux=0.0602, acc=1.000, ppl=48844.1, lr=0.00000]\n",
            "Step 20: Val Loss: 10.0672, Val Acc: 0.0164, Val PPL: 23557.78, LR: 0.01400\n",
            "\n",
            "Step 30: Val Loss: 9.0413, Val Acc: 0.0268, Val PPL: 8444.89, LR: 0.01960\n",
            "Training muon_baseline:   8% 40/500 [00:46<08:05,  1.05s/it, loss=10.7964, aux=0.0602, acc=1.000, ppl=48844.1, lr=0.00000]\n",
            "Step 40: Val Loss: 7.7168, Val Acc: 0.0948, Val PPL: 2245.77, LR: 0.02800\n",
            "\n",
            "Step 50: Val Loss: 7.4649, Val Acc: 0.0997, Val PPL: 1745.63, LR: 0.03360\n",
            "Training muon_baseline:  12% 60/500 [00:57<06:06,  1.20it/s, loss=10.7964, aux=0.0602, acc=1.000, ppl=48844.1, lr=0.00000]\n",
            "Step 60: Val Loss: 7.4503, Val Acc: 0.1063, Val PPL: 1720.46, LR: 0.04200\n",
            "\n",
            "Step 70: Val Loss: 7.3019, Val Acc: 0.1301, Val PPL: 1483.09, LR: 0.04760\n",
            "Training muon_baseline:  16% 80/500 [01:09<05:06,  1.37it/s, loss=10.7964, aux=0.0602, acc=1.000, ppl=48844.1, lr=0.00000]\n",
            "Step 80: Val Loss: 7.0643, Val Acc: 0.1368, Val PPL: 1169.42, LR: 0.05600\n",
            "\n",
            "Step 90: Val Loss: 6.8851, Val Acc: 0.1510, Val PPL: 977.60, LR: 0.06160\n",
            "Training muon_baseline:  20% 100/500 [01:21<04:29,  1.48it/s, loss=6.7440, aux=0.0701, acc=0.071, ppl=849.0, lr=0.07000]   \n",
            "Step 100: Val Loss: 6.6327, Val Acc: 0.1559, Val PPL: 759.56, LR: 0.07000\n",
            "\n",
            "Step 110: Val Loss: 6.5201, Val Acc: 0.1619, Val PPL: 678.63, LR: 0.07000\n",
            "Training muon_baseline:  24% 120/500 [01:32<04:03,  1.56it/s, loss=6.7440, aux=0.0701, acc=0.071, ppl=849.0, lr=0.07000]\n",
            "Step 120: Val Loss: 6.4086, Val Acc: 0.1635, Val PPL: 607.03, LR: 0.06998\n",
            "\n",
            "Step 130: Val Loss: 6.3288, Val Acc: 0.1654, Val PPL: 560.50, LR: 0.06997\n",
            "Training muon_baseline:  28% 140/500 [01:43<03:43,  1.61it/s, loss=6.7440, aux=0.0701, acc=0.071, ppl=849.0, lr=0.07000]\n",
            "Step 140: Val Loss: 6.2655, Val Acc: 0.1688, Val PPL: 526.10, LR: 0.06993\n",
            "\n",
            "Step 150: Val Loss: 6.2120, Val Acc: 0.1676, Val PPL: 498.72, LR: 0.06990\n",
            "Training muon_baseline:  32% 160/500 [01:55<03:25,  1.65it/s, loss=6.7440, aux=0.0701, acc=0.071, ppl=849.0, lr=0.07000]\n",
            "Step 160: Val Loss: 6.1431, Val Acc: 0.1729, Val PPL: 465.51, LR: 0.06985\n",
            "\n",
            "Step 170: Val Loss: 6.1283, Val Acc: 0.1734, Val PPL: 458.68, LR: 0.06980\n",
            "Training muon_baseline:  36% 180/500 [02:06<03:10,  1.68it/s, loss=6.7440, aux=0.0701, acc=0.071, ppl=849.0, lr=0.07000]\n",
            "Step 180: Val Loss: 6.0692, Val Acc: 0.1784, Val PPL: 432.35, LR: 0.06972\n",
            "\n",
            "Step 190: Val Loss: 6.0535, Val Acc: 0.1780, Val PPL: 425.60, LR: 0.06967\n",
            "Training muon_baseline:  40% 200/500 [02:18<02:56,  1.70it/s, loss=5.7581, aux=0.0641, acc=0.042, ppl=316.8, lr=0.06957]\n",
            "Step 200: Val Loss: 5.9757, Val Acc: 0.1861, Val PPL: 393.73, LR: 0.06957\n",
            "\n",
            "Step 210: Val Loss: 5.9508, Val Acc: 0.1879, Val PPL: 384.05, LR: 0.06950\n",
            "Training muon_baseline:  44% 220/500 [02:29<02:43,  1.71it/s, loss=5.7581, aux=0.0641, acc=0.042, ppl=316.8, lr=0.06957]\n",
            "Step 220: Val Loss: 5.8959, Val Acc: 0.1939, Val PPL: 363.55, LR: 0.06938\n",
            "\n",
            "Step 230: Val Loss: 5.8603, Val Acc: 0.1945, Val PPL: 350.83, LR: 0.06930\n",
            "Training muon_baseline:  48% 240/500 [02:41<02:31,  1.72it/s, loss=5.7581, aux=0.0641, acc=0.042, ppl=316.8, lr=0.06957]\n",
            "Step 240: Val Loss: 5.8006, Val Acc: 0.1989, Val PPL: 330.50, LR: 0.06916\n",
            "\n",
            "Step 250: Val Loss: 5.7652, Val Acc: 0.2021, Val PPL: 318.99, LR: 0.06906\n",
            "Training muon_baseline:  52% 260/500 [02:52<02:19,  1.73it/s, loss=5.7581, aux=0.0641, acc=0.042, ppl=316.8, lr=0.06957]\n",
            "Step 260: Val Loss: 5.7203, Val Acc: 0.2060, Val PPL: 305.00, LR: 0.06890\n",
            "\n",
            "Step 270: Val Loss: 5.6866, Val Acc: 0.2086, Val PPL: 294.89, LR: 0.06879\n",
            "Training muon_baseline:  56% 280/500 [03:04<02:07,  1.73it/s, loss=5.7581, aux=0.0641, acc=0.042, ppl=316.8, lr=0.06957]\n",
            "Step 280: Val Loss: 5.6400, Val Acc: 0.2104, Val PPL: 281.48, LR: 0.06862\n",
            "\n",
            "Step 290: Val Loss: 5.6110, Val Acc: 0.2142, Val PPL: 273.43, LR: 0.06849\n",
            "Training muon_baseline:  60% 300/500 [03:16<01:55,  1.73it/s, loss=5.5760, aux=0.0644, acc=0.022, ppl=264.0, lr=0.06829]\n",
            "Step 300: Val Loss: 5.5666, Val Acc: 0.2162, Val PPL: 261.55, LR: 0.06829\n",
            "\n",
            "Step 310: Val Loss: 5.5464, Val Acc: 0.2175, Val PPL: 256.32, LR: 0.06816\n",
            "Training muon_baseline:  64% 320/500 [03:27<01:43,  1.74it/s, loss=5.5760, aux=0.0644, acc=0.022, ppl=264.0, lr=0.06829]\n",
            "Step 320: Val Loss: 5.5303, Val Acc: 0.2171, Val PPL: 252.22, LR: 0.06794\n",
            "\n",
            "Step 330: Val Loss: 5.5211, Val Acc: 0.2189, Val PPL: 249.90, LR: 0.06779\n",
            "Training muon_baseline:  68% 340/500 [03:38<01:32,  1.73it/s, loss=5.5760, aux=0.0644, acc=0.022, ppl=264.0, lr=0.06829]\n",
            "Step 340: Val Loss: 5.5113, Val Acc: 0.2223, Val PPL: 247.47, LR: 0.06755\n",
            "\n",
            "Step 350: Val Loss: 5.4992, Val Acc: 0.2233, Val PPL: 244.50, LR: 0.06739\n",
            "Training muon_baseline:  72% 360/500 [03:50<01:20,  1.73it/s, loss=5.5760, aux=0.0644, acc=0.022, ppl=264.0, lr=0.06829]\n",
            "Step 360: Val Loss: 5.4816, Val Acc: 0.2255, Val PPL: 240.24, LR: 0.06713\n",
            "\n",
            "Step 370: Val Loss: 5.4649, Val Acc: 0.2284, Val PPL: 236.24, LR: 0.06696\n",
            "Training muon_baseline:  76% 380/500 [04:01<01:09,  1.73it/s, loss=5.5760, aux=0.0644, acc=0.022, ppl=264.0, lr=0.06829]\n",
            "Step 380: Val Loss: 5.4464, Val Acc: 0.2274, Val PPL: 231.93, LR: 0.06668\n",
            "\n",
            "Step 390: Val Loss: 5.4320, Val Acc: 0.2283, Val PPL: 228.61, LR: 0.06650\n",
            "Training muon_baseline:  80% 400/500 [04:13<00:57,  1.73it/s, loss=4.7150, aux=0.0655, acc=0.028, ppl=111.6, lr=0.06620]\n",
            "Step 400: Val Loss: 5.4096, Val Acc: 0.2301, Val PPL: 223.53, LR: 0.06620\n",
            "\n",
            "Step 410: Val Loss: 5.3911, Val Acc: 0.2310, Val PPL: 219.45, LR: 0.06600\n",
            "Training muon_baseline:  84% 420/500 [04:24<00:46,  1.73it/s, loss=4.7150, aux=0.0655, acc=0.028, ppl=111.6, lr=0.06620]\n",
            "Step 420: Val Loss: 5.3693, Val Acc: 0.2320, Val PPL: 214.72, LR: 0.06569\n",
            "\n",
            "Step 430: Val Loss: 5.3450, Val Acc: 0.2340, Val PPL: 209.56, LR: 0.06548\n",
            "Training muon_baseline:  88% 440/500 [04:36<00:34,  1.73it/s, loss=4.7150, aux=0.0655, acc=0.028, ppl=111.6, lr=0.06620]\n",
            "Step 440: Val Loss: 5.3203, Val Acc: 0.2362, Val PPL: 204.44, LR: 0.06515\n",
            "\n",
            "Step 450: Val Loss: 5.3055, Val Acc: 0.2371, Val PPL: 201.44, LR: 0.06493\n",
            "Training muon_baseline:  92% 460/500 [04:47<00:23,  1.73it/s, loss=4.7150, aux=0.0655, acc=0.028, ppl=111.6, lr=0.06620]\n",
            "Step 460: Val Loss: 5.2826, Val Acc: 0.2390, Val PPL: 196.89, LR: 0.06458\n",
            "\n",
            "Step 470: Val Loss: 5.2646, Val Acc: 0.2403, Val PPL: 193.36, LR: 0.06435\n",
            "Training muon_baseline:  96% 480/500 [04:59<00:11,  1.74it/s, loss=4.7150, aux=0.0655, acc=0.028, ppl=111.6, lr=0.06620]\n",
            "Step 480: Val Loss: 5.2618, Val Acc: 0.2409, Val PPL: 192.83, LR: 0.06398\n",
            "\n",
            "Step 490: Val Loss: 5.2757, Val Acc: 0.2410, Val PPL: 195.52, LR: 0.06374\n",
            "Training muon_baseline: 100% 500/500 [05:10<00:00,  1.61it/s, loss=4.7150, aux=0.0655, acc=0.028, ppl=111.6, lr=0.06620]\n",
            "\n",
            "Final Results:\n",
            "  Val Loss: 5.2897\n",
            "Val Accuracy: 0.2403\n",
            "   Val Perplexity: 198.29\n",
            "   Total Time: 5.22 min\n",
            "  Metrics saved to muon_baseline/metrics.json\n",
            "  Plots saved to muon_baseline/metrics_plot.png\n",
            "Model saved to muon_baseline/model.pt\n",
            "\n",
            "Experiment 'muon_baseline' completed in 5.36 minutes\n",
            "2025-12-16 22:06:29 | INFO     | root | Experiment 'muon_baseline' completed. Final val loss: 5.2897\n",
            "\n",
            "================================================================================\n",
            "Starting experiment 2/3: adam_baseline\n",
            "Starting experiment 2/3: adam_baseline\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            " Running Experiment: adam_baseline\n",
            "================================================================================\n",
            "Description: Pure Adam: AdamW for all parameters - LR 0.001 (optimal)\n",
            "Optimizer: adam\n",
            "Output: adam_baseline\n",
            "================================================================================\n",
            "\n",
            "ðŸŒ± Set all seeds to 42\n",
            "   Total parameters: 79,059,840\n",
            "   Active parameters: 22,436,736\n",
            "  Expert parameters: 56,623,104\n",
            "  Parameter efficiency: 28.4% active per forward pass\n",
            "  AdamW parameters (all): 79,059,840\n",
            "  Using Adam optimizer\n",
            "Training adam_baseline:   0% 0/500 [00:00<?, ?it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 10: Val Loss: 10.7261, Val Acc: 0.0164, Val PPL: 45526.80, LR: 0.00008\n",
            "Training adam_baseline:   4% 20/500 [00:08<03:32,  2.26it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 20: Val Loss: 10.2515, Val Acc: 0.0329, Val PPL: 28323.69, LR: 0.00020\n",
            "\n",
            "Step 30: Val Loss: 9.9266, Val Acc: 0.0787, Val PPL: 20467.06, LR: 0.00028\n",
            "Training adam_baseline:   8% 40/500 [00:20<03:56,  1.95it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 40: Val Loss: 9.5573, Val Acc: 0.0710, Val PPL: 14147.83, LR: 0.00040\n",
            "\n",
            "Step 50: Val Loss: 9.2608, Val Acc: 0.0737, Val PPL: 10517.52, LR: 0.00048\n",
            "Training adam_baseline:  12% 60/500 [00:31<03:56,  1.86it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 60: Val Loss: 8.7386, Val Acc: 0.0873, Val PPL: 6239.30, LR: 0.00060\n",
            "\n",
            "Step 70: Val Loss: 8.3940, Val Acc: 0.0880, Val PPL: 4420.47, LR: 0.00068\n",
            "Training adam_baseline:  16% 80/500 [00:42<03:49,  1.83it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 80: Val Loss: 7.9367, Val Acc: 0.0886, Val PPL: 2798.24, LR: 0.00080\n",
            "\n",
            "Step 90: Val Loss: 7.7105, Val Acc: 0.0891, Val PPL: 2231.73, LR: 0.00088\n",
            "Training adam_baseline:  20% 100/500 [00:54<03:40,  1.82it/s, loss=7.6147, aux=0.0616, acc=0.062, ppl=2027.7, lr=0.00100]  \n",
            "Step 100: Val Loss: 7.5445, Val Acc: 0.0920, Val PPL: 1890.27, LR: 0.00100\n",
            "\n",
            "Step 110: Val Loss: 7.5177, Val Acc: 0.0960, Val PPL: 1840.28, LR: 0.00100\n",
            "Training adam_baseline:  24% 120/500 [01:05<03:30,  1.81it/s, loss=7.6147, aux=0.0616, acc=0.062, ppl=2027.7, lr=0.00100]\n",
            "Step 120: Val Loss: 7.4914, Val Acc: 0.0957, Val PPL: 1792.64, LR: 0.00100\n",
            "\n",
            "Step 130: Val Loss: 7.4139, Val Acc: 0.1008, Val PPL: 1658.88, LR: 0.00100\n",
            "Training adam_baseline:  28% 140/500 [01:16<03:19,  1.80it/s, loss=7.6147, aux=0.0616, acc=0.062, ppl=2027.7, lr=0.00100]\n",
            "Step 140: Val Loss: 7.3424, Val Acc: 0.1016, Val PPL: 1544.42, LR: 0.00100\n",
            "\n",
            "Step 150: Val Loss: 7.2855, Val Acc: 0.1031, Val PPL: 1459.06, LR: 0.00100\n",
            "Training adam_baseline:  32% 160/500 [01:27<03:07,  1.81it/s, loss=7.6147, aux=0.0616, acc=0.062, ppl=2027.7, lr=0.00100]\n",
            "Step 160: Val Loss: 7.2058, Val Acc: 0.1064, Val PPL: 1347.22, LR: 0.00100\n",
            "\n",
            "Step 170: Val Loss: 7.1846, Val Acc: 0.1094, Val PPL: 1318.99, LR: 0.00100\n",
            "Training adam_baseline:  36% 180/500 [01:38<02:57,  1.80it/s, loss=7.6147, aux=0.0616, acc=0.062, ppl=2027.7, lr=0.00100]\n",
            "Step 180: Val Loss: 7.1795, Val Acc: 0.1105, Val PPL: 1312.25, LR: 0.00100\n",
            "\n",
            "Step 190: Val Loss: 7.0811, Val Acc: 0.1164, Val PPL: 1189.32, LR: 0.00100\n",
            "Training adam_baseline:  40% 200/500 [01:49<02:46,  1.81it/s, loss=7.1513, aux=0.0641, acc=0.070, ppl=1275.8, lr=0.00099]\n",
            "Step 200: Val Loss: 7.0070, Val Acc: 0.1231, Val PPL: 1104.31, LR: 0.00099\n",
            "\n",
            "Step 210: Val Loss: 6.9709, Val Acc: 0.1308, Val PPL: 1065.19, LR: 0.00099\n",
            "Training adam_baseline:  44% 220/500 [02:00<02:35,  1.80it/s, loss=7.1513, aux=0.0641, acc=0.070, ppl=1275.8, lr=0.00099]\n",
            "Step 220: Val Loss: 6.9082, Val Acc: 0.1383, Val PPL: 1000.42, LR: 0.00099\n",
            "\n",
            "Step 230: Val Loss: 6.8278, Val Acc: 0.1443, Val PPL: 923.15, LR: 0.00099\n",
            "Training adam_baseline:  48% 240/500 [02:11<02:24,  1.80it/s, loss=7.1513, aux=0.0641, acc=0.070, ppl=1275.8, lr=0.00099]\n",
            "Step 240: Val Loss: 6.7610, Val Acc: 0.1482, Val PPL: 863.49, LR: 0.00099\n",
            "\n",
            "Step 250: Val Loss: 6.7140, Val Acc: 0.1543, Val PPL: 823.86, LR: 0.00099\n",
            "Training adam_baseline:  52% 260/500 [02:22<02:13,  1.80it/s, loss=7.1513, aux=0.0641, acc=0.070, ppl=1275.8, lr=0.00099]\n",
            "Step 260: Val Loss: 6.6493, Val Acc: 0.1586, Val PPL: 772.24, LR: 0.00098\n",
            "\n",
            "Step 270: Val Loss: 6.6053, Val Acc: 0.1601, Val PPL: 738.98, LR: 0.00098\n",
            "Training adam_baseline:  56% 280/500 [02:33<02:02,  1.80it/s, loss=7.1513, aux=0.0641, acc=0.070, ppl=1275.8, lr=0.00099]\n",
            "Step 280: Val Loss: 6.5486, Val Acc: 0.1646, Val PPL: 698.23, LR: 0.00098\n",
            "\n",
            "Step 290: Val Loss: 6.5117, Val Acc: 0.1661, Val PPL: 672.99, LR: 0.00098\n",
            "Training adam_baseline:  60% 300/500 [02:45<01:50,  1.80it/s, loss=6.4559, aux=0.0620, acc=0.064, ppl=636.5, lr=0.00098] \n",
            "Step 300: Val Loss: 6.4617, Val Acc: 0.1707, Val PPL: 640.15, LR: 0.00098\n",
            "\n",
            "Step 310: Val Loss: 6.4297, Val Acc: 0.1740, Val PPL: 619.98, LR: 0.00097\n",
            "Training adam_baseline:  64% 320/500 [02:55<01:39,  1.81it/s, loss=6.4559, aux=0.0620, acc=0.064, ppl=636.5, lr=0.00098]\n",
            "Step 320: Val Loss: 6.3762, Val Acc: 0.1764, Val PPL: 587.71, LR: 0.00097\n",
            "\n",
            "Step 330: Val Loss: 6.3471, Val Acc: 0.1771, Val PPL: 570.86, LR: 0.00097\n",
            "Training adam_baseline:  68% 340/500 [03:06<01:28,  1.81it/s, loss=6.4559, aux=0.0620, acc=0.064, ppl=636.5, lr=0.00098]\n",
            "Step 340: Val Loss: 6.3148, Val Acc: 0.1774, Val PPL: 552.70, LR: 0.00097\n",
            "\n",
            "Step 350: Val Loss: 6.2842, Val Acc: 0.1806, Val PPL: 536.03, LR: 0.00096\n",
            "Training adam_baseline:  72% 360/500 [03:17<01:17,  1.81it/s, loss=6.4559, aux=0.0620, acc=0.064, ppl=636.5, lr=0.00098]\n",
            "Step 360: Val Loss: 6.2481, Val Acc: 0.1815, Val PPL: 517.01, LR: 0.00096\n",
            "\n",
            "Step 370: Val Loss: 6.2203, Val Acc: 0.1835, Val PPL: 502.84, LR: 0.00096\n",
            "Training adam_baseline:  76% 380/500 [03:29<01:06,  1.81it/s, loss=6.4559, aux=0.0620, acc=0.064, ppl=636.5, lr=0.00098]\n",
            "Step 380: Val Loss: 6.1842, Val Acc: 0.1857, Val PPL: 485.04, LR: 0.00095\n",
            "\n",
            "Step 390: Val Loss: 6.1608, Val Acc: 0.1868, Val PPL: 473.80, LR: 0.00095\n",
            "Training adam_baseline:  80% 400/500 [03:40<00:55,  1.81it/s, loss=5.9054, aux=0.0607, acc=0.041, ppl=367.0, lr=0.00095]\n",
            "Step 400: Val Loss: 6.1234, Val Acc: 0.1889, Val PPL: 456.42, LR: 0.00095\n",
            "\n",
            "Step 410: Val Loss: 6.1211, Val Acc: 0.1893, Val PPL: 455.37, LR: 0.00094\n",
            "Training adam_baseline:  84% 420/500 [03:51<00:44,  1.81it/s, loss=5.9054, aux=0.0607, acc=0.041, ppl=367.0, lr=0.00095]\n",
            "Step 420: Val Loss: 6.0731, Val Acc: 0.1906, Val PPL: 434.03, LR: 0.00094\n",
            "\n",
            "Step 430: Val Loss: 6.0475, Val Acc: 0.1921, Val PPL: 423.05, LR: 0.00094\n",
            "Training adam_baseline:  88% 440/500 [04:02<00:33,  1.81it/s, loss=5.9054, aux=0.0607, acc=0.041, ppl=367.0, lr=0.00095]\n",
            "Step 440: Val Loss: 6.0043, Val Acc: 0.1958, Val PPL: 405.17, LR: 0.00093\n",
            "\n",
            "Step 450: Val Loss: 5.9875, Val Acc: 0.1960, Val PPL: 398.43, LR: 0.00093\n",
            "Training adam_baseline:  92% 460/500 [04:13<00:22,  1.81it/s, loss=5.9054, aux=0.0607, acc=0.041, ppl=367.0, lr=0.00095]\n",
            "Step 460: Val Loss: 5.9513, Val Acc: 0.1974, Val PPL: 384.27, LR: 0.00092\n",
            "\n",
            "Step 470: Val Loss: 5.9383, Val Acc: 0.1990, Val PPL: 379.30, LR: 0.00092\n",
            "Training adam_baseline:  96% 480/500 [04:23<00:10,  1.82it/s, loss=5.9054, aux=0.0607, acc=0.041, ppl=367.0, lr=0.00095]\n",
            "Step 480: Val Loss: 5.9131, Val Acc: 0.1987, Val PPL: 369.85, LR: 0.00091\n",
            "\n",
            "Step 490: Val Loss: 5.8948, Val Acc: 0.2019, Val PPL: 363.13, LR: 0.00091\n",
            "Training adam_baseline: 100% 500/500 [04:35<00:00,  1.82it/s, loss=5.9054, aux=0.0607, acc=0.041, ppl=367.0, lr=0.00095]\n",
            "\n",
            "Final Results:\n",
            "  Val Loss: 5.8779\n",
            "Val Accuracy: 0.2010\n",
            "   Val Perplexity: 357.05\n",
            "   Total Time: 4.62 min\n",
            "  Metrics saved to adam_baseline/metrics.json\n",
            "  Plots saved to adam_baseline/metrics_plot.png\n",
            "Model saved to adam_baseline/model.pt\n",
            "\n",
            "Experiment 'adam_baseline' completed in 4.77 minutes\n",
            "2025-12-16 22:11:16 | INFO     | root | Experiment 'adam_baseline' completed. Final val loss: 5.8779\n",
            "\n",
            "================================================================================\n",
            "Starting experiment 3/3: adam_higher_lr\n",
            "Starting experiment 3/3: adam_higher_lr\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            " Running Experiment: adam_higher_lr\n",
            "================================================================================\n",
            "Description: Adam with higher learning rate (0.002)\n",
            "Optimizer: adam\n",
            "Output: adam_higher_lr\n",
            "================================================================================\n",
            "\n",
            "ðŸŒ± Set all seeds to 42\n",
            "   Total parameters: 79,059,840\n",
            "   Active parameters: 22,436,736\n",
            "  Expert parameters: 56,623,104\n",
            "  Parameter efficiency: 28.4% active per forward pass\n",
            "  AdamW parameters (all): 79,059,840\n",
            "  Using Adam optimizer\n",
            "Training adam_higher_lr:   0% 0/500 [00:00<?, ?it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 10: Val Loss: 10.6692, Val Acc: 0.0164, Val PPL: 43010.32, LR: 0.00016\n",
            "Training adam_higher_lr:   4% 20/500 [00:09<03:37,  2.20it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 20: Val Loss: 9.9617, Val Acc: 0.0774, Val PPL: 21198.53, LR: 0.00040\n",
            "\n",
            "Step 30: Val Loss: 9.6243, Val Acc: 0.0747, Val PPL: 15127.94, LR: 0.00056\n",
            "Training adam_higher_lr:   8% 40/500 [00:20<03:58,  1.93it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 40: Val Loss: 8.9806, Val Acc: 0.0845, Val PPL: 7947.01, LR: 0.00080\n",
            "\n",
            "Step 50: Val Loss: 8.4921, Val Acc: 0.0872, Val PPL: 4876.20, LR: 0.00096\n",
            "Training adam_higher_lr:  12% 60/500 [00:31<03:56,  1.86it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 60: Val Loss: 7.8721, Val Acc: 0.0881, Val PPL: 2623.07, LR: 0.00120\n",
            "\n",
            "Step 70: Val Loss: 7.6351, Val Acc: 0.0877, Val PPL: 2069.55, LR: 0.00136\n",
            "Training adam_higher_lr:  16% 80/500 [00:42<03:49,  1.83it/s, loss=10.7884, aux=0.0602, acc=1.000, ppl=48454.2, lr=0.00000]\n",
            "Step 80: Val Loss: 7.8133, Val Acc: 0.0915, Val PPL: 2473.23, LR: 0.00160\n",
            "\n",
            "Step 90: Val Loss: 7.6348, Val Acc: 0.0813, Val PPL: 2068.91, LR: 0.00176\n",
            "Training adam_higher_lr:  20% 100/500 [00:54<03:40,  1.81it/s, loss=7.6552, aux=0.0743, acc=0.064, ppl=2111.6, lr=0.00200]  \n",
            "Step 100: Val Loss: 7.5862, Val Acc: 0.0928, Val PPL: 1970.84, LR: 0.00200\n",
            "\n",
            "Step 110: Val Loss: 7.7493, Val Acc: 0.0788, Val PPL: 2319.96, LR: 0.00200\n",
            "Training adam_higher_lr:  24% 120/500 [01:05<03:29,  1.81it/s, loss=7.6552, aux=0.0743, acc=0.064, ppl=2111.6, lr=0.00200]\n",
            "Step 120: Val Loss: 7.4699, Val Acc: 0.0949, Val PPL: 1754.49, LR: 0.00200\n",
            "\n",
            "Step 130: Val Loss: 7.3925, Val Acc: 0.0968, Val PPL: 1623.70, LR: 0.00200\n",
            "Training adam_higher_lr:  28% 140/500 [01:15<03:18,  1.82it/s, loss=7.6552, aux=0.0743, acc=0.064, ppl=2111.6, lr=0.00200]\n",
            "Step 140: Val Loss: 7.3566, Val Acc: 0.0961, Val PPL: 1566.52, LR: 0.00200\n",
            "\n",
            "Step 150: Val Loss: 7.2967, Val Acc: 0.0990, Val PPL: 1475.48, LR: 0.00200\n",
            "Training adam_higher_lr:  32% 160/500 [01:26<03:06,  1.82it/s, loss=7.6552, aux=0.0743, acc=0.064, ppl=2111.6, lr=0.00200]\n",
            "Step 160: Val Loss: 7.2317, Val Acc: 0.1008, Val PPL: 1382.63, LR: 0.00200\n",
            "\n",
            "Step 170: Val Loss: 7.2127, Val Acc: 0.1008, Val PPL: 1356.51, LR: 0.00199\n",
            "Training adam_higher_lr:  36% 180/500 [01:38<02:57,  1.80it/s, loss=7.6552, aux=0.0743, acc=0.064, ppl=2111.6, lr=0.00200]\n",
            "Step 180: Val Loss: 7.1757, Val Acc: 0.1036, Val PPL: 1307.23, LR: 0.00199\n",
            "\n",
            "Step 190: Val Loss: 7.1753, Val Acc: 0.0985, Val PPL: 1306.75, LR: 0.00199\n",
            "Training adam_higher_lr:  40% 200/500 [01:49<02:46,  1.80it/s, loss=7.2309, aux=0.0685, acc=0.050, ppl=1381.4, lr=0.00199]\n",
            "Step 200: Val Loss: 7.0885, Val Acc: 0.1060, Val PPL: 1198.14, LR: 0.00199\n",
            "\n",
            "Step 210: Val Loss: 7.0820, Val Acc: 0.1060, Val PPL: 1190.37, LR: 0.00199\n",
            "Training adam_higher_lr:  44% 220/500 [02:00<02:36,  1.79it/s, loss=7.2309, aux=0.0685, acc=0.050, ppl=1381.4, lr=0.00199]\n",
            "Step 220: Val Loss: 7.0160, Val Acc: 0.1087, Val PPL: 1114.35, LR: 0.00198\n",
            "\n",
            "Step 230: Val Loss: 7.0297, Val Acc: 0.1091, Val PPL: 1129.66, LR: 0.00198\n",
            "Training adam_higher_lr:  48% 240/500 [02:11<02:25,  1.79it/s, loss=7.2309, aux=0.0685, acc=0.050, ppl=1381.4, lr=0.00199]\n",
            "Step 240: Val Loss: 6.9379, Val Acc: 0.1121, Val PPL: 1030.63, LR: 0.00198\n",
            "\n",
            "Step 250: Val Loss: 6.8931, Val Acc: 0.1192, Val PPL: 985.50, LR: 0.00197\n",
            "Training adam_higher_lr:  52% 260/500 [02:23<02:13,  1.79it/s, loss=7.2309, aux=0.0685, acc=0.050, ppl=1381.4, lr=0.00199]\n",
            "Step 260: Val Loss: 6.8495, Val Acc: 0.1223, Val PPL: 943.39, LR: 0.00197\n",
            "\n",
            "Step 270: Val Loss: 6.8095, Val Acc: 0.1238, Val PPL: 906.43, LR: 0.00197\n",
            "Training adam_higher_lr:  56% 280/500 [02:34<02:02,  1.80it/s, loss=7.2309, aux=0.0685, acc=0.050, ppl=1381.4, lr=0.00199]\n",
            "Step 280: Val Loss: 6.7570, Val Acc: 0.1306, Val PPL: 860.09, LR: 0.00196\n",
            "\n",
            "Step 290: Val Loss: 6.7145, Val Acc: 0.1363, Val PPL: 824.23, LR: 0.00196\n",
            "Training adam_higher_lr:  60% 300/500 [02:45<01:51,  1.80it/s, loss=6.6459, aux=0.0639, acc=0.056, ppl=769.6, lr=0.00195] \n",
            "Step 300: Val Loss: 6.6728, Val Acc: 0.1434, Val PPL: 790.62, LR: 0.00195\n",
            "\n",
            "Step 310: Val Loss: 6.6190, Val Acc: 0.1474, Val PPL: 749.17, LR: 0.00195\n",
            "Training adam_higher_lr:  64% 320/500 [02:56<01:39,  1.81it/s, loss=6.6459, aux=0.0639, acc=0.056, ppl=769.6, lr=0.00195]\n",
            "Step 320: Val Loss: 6.6053, Val Acc: 0.1506, Val PPL: 739.00, LR: 0.00194\n",
            "\n",
            "Step 330: Val Loss: 6.5630, Val Acc: 0.1541, Val PPL: 708.36, LR: 0.00194\n",
            "Training adam_higher_lr:  68% 340/500 [03:07<01:28,  1.80it/s, loss=6.6459, aux=0.0639, acc=0.056, ppl=769.6, lr=0.00195]\n",
            "Step 340: Val Loss: 6.5303, Val Acc: 0.1571, Val PPL: 685.60, LR: 0.00193\n",
            "\n",
            "Step 350: Val Loss: 6.4826, Val Acc: 0.1568, Val PPL: 653.66, LR: 0.00193\n",
            "Training adam_higher_lr:  72% 360/500 [03:18<01:17,  1.80it/s, loss=6.6459, aux=0.0639, acc=0.056, ppl=769.6, lr=0.00195]\n",
            "Step 360: Val Loss: 6.4484, Val Acc: 0.1588, Val PPL: 631.66, LR: 0.00192\n",
            "\n",
            "Step 370: Val Loss: 6.4193, Val Acc: 0.1617, Val PPL: 613.58, LR: 0.00191\n",
            "Training adam_higher_lr:  76% 380/500 [03:29<01:06,  1.80it/s, loss=6.6459, aux=0.0639, acc=0.056, ppl=769.6, lr=0.00195]\n",
            "Step 380: Val Loss: 6.3900, Val Acc: 0.1624, Val PPL: 595.88, LR: 0.00191\n",
            "\n",
            "Step 390: Val Loss: 6.3737, Val Acc: 0.1641, Val PPL: 586.23, LR: 0.00190\n",
            "Training adam_higher_lr:  80% 400/500 [03:40<00:55,  1.80it/s, loss=6.1204, aux=0.0622, acc=0.034, ppl=455.1, lr=0.00189]\n",
            "Step 400: Val Loss: 6.3391, Val Acc: 0.1674, Val PPL: 566.30, LR: 0.00189\n",
            "\n",
            "Step 410: Val Loss: 6.2926, Val Acc: 0.1705, Val PPL: 540.56, LR: 0.00189\n",
            "Training adam_higher_lr:  84% 420/500 [03:51<00:44,  1.80it/s, loss=6.1204, aux=0.0622, acc=0.034, ppl=455.1, lr=0.00189]\n",
            "Step 420: Val Loss: 6.2528, Val Acc: 0.1731, Val PPL: 519.48, LR: 0.00188\n",
            "\n",
            "Step 430: Val Loss: 6.2331, Val Acc: 0.1741, Val PPL: 509.36, LR: 0.00187\n",
            "Training adam_higher_lr:  88% 440/500 [04:02<00:33,  1.80it/s, loss=6.1204, aux=0.0622, acc=0.034, ppl=455.1, lr=0.00189]\n",
            "Step 440: Val Loss: 6.2033, Val Acc: 0.1744, Val PPL: 494.40, LR: 0.00186\n",
            "\n",
            "Step 450: Val Loss: 6.1791, Val Acc: 0.1766, Val PPL: 482.55, LR: 0.00186\n",
            "Training adam_higher_lr:  92% 460/500 [04:13<00:22,  1.80it/s, loss=6.1204, aux=0.0622, acc=0.034, ppl=455.1, lr=0.00189]\n",
            "Step 460: Val Loss: 6.1367, Val Acc: 0.1797, Val PPL: 462.50, LR: 0.00185\n",
            "\n",
            "Step 470: Val Loss: 6.1141, Val Acc: 0.1819, Val PPL: 452.17, LR: 0.00184\n",
            "Training adam_higher_lr:  96% 480/500 [04:24<00:10,  1.82it/s, loss=6.1204, aux=0.0622, acc=0.034, ppl=455.1, lr=0.00189]\n",
            "Step 480: Val Loss: 6.0930, Val Acc: 0.1832, Val PPL: 442.76, LR: 0.00183\n",
            "\n",
            "Step 490: Val Loss: 6.0805, Val Acc: 0.1833, Val PPL: 437.23, LR: 0.00182\n",
            "Training adam_higher_lr: 100% 500/500 [04:35<00:00,  1.81it/s, loss=6.1204, aux=0.0622, acc=0.034, ppl=455.1, lr=0.00189]\n",
            "\n",
            "Final Results:\n",
            "  Val Loss: 6.0684\n",
            "Val Accuracy: 0.1853\n",
            "   Val Perplexity: 431.97\n",
            "   Total Time: 4.63 min\n",
            "  Metrics saved to adam_higher_lr/metrics.json\n",
            "  Plots saved to adam_higher_lr/metrics_plot.png\n",
            "Model saved to adam_higher_lr/model.pt\n",
            "\n",
            "Experiment 'adam_higher_lr' completed in 4.76 minutes\n",
            "2025-12-16 22:16:03 | INFO     | root | Experiment 'adam_higher_lr' completed. Final val loss: 6.0684\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPARISON: MUON VS ADAM\n",
            "================================================================================\n",
            "\n",
            "Experiment                Optimizer         Final Loss    Best Loss    Final Acc   Time (min)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "muon_baseline             muon_hybrid           5.2897       5.2618       0.2403         5.36\n",
            "adam_baseline             adam                  5.8779       5.8779       0.2010         4.77\n",
            "adam_higher_lr            adam                  6.0684       6.0684       0.1853         4.76\n",
            "\n",
            "Best Muon: muon_baseline (loss: 5.2618)\n",
            "Best Adam: adam_baseline (loss: 5.8779)\n",
            "\n",
            "Muon is better by 10.48%\n",
            "\n",
            "Comparison plot saved to comparison_plot.png\n",
            " Comparison summary saved to comparison_summary.json\n",
            "\n",
            "================================================================================\n",
            "Completed 3/3 experiments\n",
            "================================================================================\n",
            "[W1216 22:16:08.916876177 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
          ]
        }
      ]
    }
  ]
}